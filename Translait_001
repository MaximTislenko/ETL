https://sparkbyexamples.com/spark/sparksession-explained-with-examples/
Что такое SparkSession | Точка входа

  Чтобы создать SparkSession в Scala или Python, вам необходимо использовать метод шаблона компоновщика builder() и вызвать метод getOrCreate().
Он возвращает уже существующий SparkSession; в противном случае создается новый SparkSession. В приведенном ниже примере создается SparkSession в Scala.
  SparkSession объединяет несколько ранее отдельных контекстов, таких как SQLContext, HiveContext и StreamingContext, в одну точку входа, 
упрощая взаимодействие со Spark и его различными API. Он позволяет пользователям выполнять различные операции, такие как чтение данных 
из различных источников, выполнение SQL-запросов, создание фреймов данных и наборов данных, а также эффективное выполнение действий
с распределенными наборами данных.
  Для тех, кто работает со Spark через CLI искровой оболочки, переменная «spark» автоматически предоставляет сеанс Spark по умолчанию,
устраняя необходимость создания вручную в этом контексте. 
  spark-shell(https://sparkbyexamples.com/spark/sparksession-explained-with-examples/#:~:text=Spark%20through%20the-,spark%2Dshell,-CLI%2C%20the%20%E2%80%98spark)

В этой статье я углублюсь в суть SparkSession, в то, как создать объект SparkSession, и изучу его часто используемые методы.

Что такое SparkSession.
  SparkSession был представлен в версии Spark 2.0. Это точка входа в базовые функции Spark для программного создания Spark RDD, DataFrame и DataSet.
Объект SparkSession — это переменная по умолчанию, доступная в искровой оболочке, и ее можно создать программно с помощью шаблона компоновщика SparkSession.
  Если вы ищете объяснение PySpark, обратитесь к разделу «Как создать SparkSession в PySpark». (https://sparkbyexamples.com/pyspark/pyspark-what-is-sparksession/)

1. Введение в SparkSession
  Как упоминалось вначале, SparkSession — это точка входа в Spark, и создание экземпляра SparkSession будет первым оператором, который вы напишете в программе с помощью
RDD (https://sparkbyexamples.com/spark-rdd-tutorial/), DataFrame (https://sparkbyexamples.com/spark-dataframe-tutorial/) и Dataset.
SparkSession будет создан с использованием шаблона компоновщика SparkSession.builder().
  До Spark 2.0 точкой входа была SparkContext (https://sparkbyexamples.com/spark/spark-sparkcontext/), и она не была полностью заменена SparkSession.
Многие функции SparkContext по-прежнему доступны и используются в Spark 2.0 и более поздних версиях. Вы также должны знать, 
что SparkSession внутренне создает SparkConfig и SparkContext с конфигурацией, предоставленной SparkSession.
  В Spark 2.0 появился новый класс org.apache.spark.sql.SparkSession, который представляет собой комбинированный класс для всех различных контекстов, 
которые у нас были до версии 2.0 (SQLContext, HiveContext и т. д.); следовательно, сеанс Spark можно использовать вместо SQLContext, HiveContext и других контекстов.
  Spark Session также включает в себя все API, доступные в разных контекстах:
->SparkContext
->SQLContext
->StreamingContext
->HiveContext
Сколько сеансов SparkSession можно создать в приложении?
  В приложении Spark вы можете создать столько SparkSession, сколько захотите, используя SparkSession.builder() или SparkSession.newSession().
Многие объекты сеанса Spark необходимы, если вы хотите логически разделить таблицы Spark (реляционные сущности).

2. SparkSession в spark-shell (искровой оболочке)
  По умолчанию оболочка Spark предоставляет объект Spark, который является экземпляром класса SparkSession.
Мы можем напрямую использовать этот объект, когда это необходимо, в spark-shell (искровой оболочке) 
(https://sparkbyexamples.com/spark/spark-shell-usage-with-examples/).

// Usage of spark variable
scala> spark.version

Как и в оболочке Spark, в большинстве инструментов, записных книжек и Azure Databricks среда создает объект SparkSession по умолчанию, 
который мы можем использовать, поэтому вам не придется беспокоиться о создании сеанса Spark.

3. Как создать SparkSession
  Создание SparkSession имеет фундаментальное значение, поскольку оно инициализирует среду, необходимую для использования возможностей Apache Spark.
   Чтобы создать SparkSession в Scala или Python, вам необходимо использовать метод шаблона компоновщика builder() и вызвать метод getOrCreate().
Он возвращает уже существующий SparkSession; в противном случае создается новый SparkSession. В приведенном ниже примере создается SparkSession в Scala.

// Create SparkSession object
import org.apache.spark.sql.SparkSession
object SparkSessionTest extends App {
  val spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExamples.com")
      .getOrCreate();
  println(spark)
  println("Spark Version : "+spark.version)
}

// Outputs
// org.apache.spark.sql.SparkSession@2fdf17dc
// Spark Version : 3.4.1

Из приведенного выше кода :–)
**SparkSession.builder() – Возвращает класс SparkSession.Builder. Это конструктор для SparkSession. master(), appName() и getOrCreate() — это методы SparkSession.Builder.
**master() — позволяет приложениям Spark подключаться и работать в разных режимах (локальный, автономный кластер, Mesos, YARN), в зависимости от конфигурации.:
    ~Используйте local[x] при работе на локальном ноутбуке. x должен быть целочисленным значением и должен быть больше 0; это показывает, 
сколько разделов следует создать при использовании RDD, DataFrame и Dataset. В идеале значение x должно соответствовать количеству имеющихся у вас ядер ЦП.
    ~Для автономного использования используйте spark://master:7077.
**appName()  – задает имя приложения Spark, которое отображается в веб-интерфейсе Spark. Если имя приложения не задано, оно устанавливает случайное имя.
**getOrCreate() – возвращает объект SparkSession, если он уже существует. Создает новый, если он не существует.

3.1 Получение существующего сеанса SparkSession
  Вы можете получить существующий SparkSession в Scala программно, используя приведенный ниже пример. Чтобы получить существующий SparkSession, вам не нужно указывать имя приложения, мастер и т. д.

// Get existing SparkSession 
import org.apache.spark.sql.SparkSession
val spark2 = SparkSession.builder().getOrCreate()
print(spark2)

// Output:
// org.apache.spark.sql.SparkSession@2fdf17dc

Сравните хеш объекта spark и spark2. Поскольку он вернул существующий сеанс, оба объекта имеют одинаковое значение хеш-функции.

3.2 Создайте еще один сеанс SparkSession
  Иногда вам может потребоваться создать несколько сеансов, чего можно легко добиться с помощью метода newSession(). При этом используется то же имя приложения и мастер, что и в существующем сеансе. 
Базовый SparkContext будет одинаковым для обоих сеансов, поскольку для каждого приложения Spark может быть только один контекст.


// Create a new SparkSession
val spark3 = spark.newSession()
print(spark3)

// Output:
// org.apache.spark.sql.SparkSession@692dba54

Сравните этот хеш с хешем из приведенного выше примера; оно должно быть другим.

3.4 Создание SparkSession с включением Hive
  Чтобы использовать Hive со Spark - (https://sparkbyexamples.com/spark/spark-sql-read-hive-table/), вам необходимо включить его 
с помощью метода EnableHiveSupport() - (https://sparkbyexamples.com/spark/spark-enable-hive-support/). SparkSession из Spark2.0 обеспечивает встроенную поддержку операций Hive, таких как написание 
запросов к таблицам Hive с использованием HQL, доступ к пользовательским функциям Hive и чтение данных из таблиц Hive.

// Enabling Hive to use in Spark
val spark = SparkSession.builder()
      .master("local[1]")
      .appName("SparkByExamples.com")
      .config("spark.sql.warehouse.dir", "<path>/spark-warehouse")
      .enableHiveSupport()
      .getOrCreate();


4. Другие варианты использования SparkSession
4.1 Установка и получение всех конфигураций Spark
    После создания SparkSession вы можете добавить конфигурации Spark во время выполнения или получить все конфигурации.

// Set Config
spark.conf.set("spark.sql.shuffle.partitions", "30")

// Get all Spark Configs
val configMap:Map[String, String] = spark.conf.getAll

4.2 Создать фрейм данных
    SparkSession также предоставляет несколько методов для создания кадра данных и 
набора данных Spark (https://sparkbyexamples.com/spark/different-ways-to-create-a-spark-dataframe/). 
В приведенном ниже примере используется метод createDataFrame(), который принимает список данных.

// Create DataFrame
val df = spark.createDataFrame(
    List(("Scala", 25000), ("Spark", 35000), ("PHP", 21000)))
df.show()

// Output:
// +-----+-----+
// |   _1|   _2|
// +-----+-----+
// |Scala|25000|
// |Spark|35000|
// |  PHP|21000|
// +-----+-----+

4.3 Работа со Spark SQL
    Используя SparkSession, вы можете получить доступ к возможностям Spark SQL в Apache Spark. Чтобы сначала использовать функции SQL, 
вам необходимо создать временное представление в Spark (https://sparkbyexamples.com/spark/spark-createorreplacetempview-vs-registertemptable/). 
Получив временное представление, вы можете запускать любые запросы ANSI SQL, используя метод spark.sql().

// Spark SQL
df.createOrReplaceTempView("sample_table")
val df2 = spark.sql("SELECT _1,_2 FROM sample_table")
df2.show()

Временные представления Spark SQL относятся к области сеанса и будут недоступны, если сеанс, создавший их, завершится. 
Если вы хотите иметь временное представление, которое используется всеми сеансами и сохраняется до тех пор, 
пока приложение Spark не завершится, вы можете создать глобальное временное представление с помощью createGlobalTempView().

4.4 Создание таблицы ульев (таблиц Hive)
    Как объяснялось выше, SparkSession также можно использовать для создания таблиц Hive и выполнения запросов к ним. 
Обратите внимание: для тестирования вам не обязательно устанавливать Hive. saveAsTable() создает управляемую таблицу Hive. 
Запросите таблицу с помощью spark.sql().

// Create Hive table & query it.  
spark.table("sample_table").write.saveAsTable("sample_hive_table")
val df3 = spark.sql("SELECT _1,_2 FROM sample_hive_table")
df3.show()

4.5 Работа с каталогами
    Чтобы получить метаданные каталога, Spark Session предоставляет переменную каталога. 
Обратите внимание, что эти методы spark.catalog.listDatabases и spark.catalog.listTables возвращают набор данных.

// Get metadata from the Catalog
// List databases
val ds = spark.catalog.listDatabases
ds.show(false)

// Output:
// +-------+----------------+----------------------------+
// |name   |description     |locationUri                 |
// +-------+----------------+----------------------------+
// |default|default database|file:/<path>/spark-warehouse|
// +-------+----------------+----------------------------+

// List Tables
val ds2 = spark.catalog.listTables
ds2.show(false)

// Output:
// +-----------------+--------+-----------+---------+-----------+
// |name             |database|description|tableType|isTemporary|
// +-----------------+--------+-----------+---------+-----------+
// |sample_hive_table|default |null       |MANAGED  |false      |
// |sample_table     |null    |null       |TEMPORARY|true       |
// +-----------------+--------+-----------+---------+-----------+

    Обратите внимание на две таблицы, которые мы создали на данный момент: 
таблица sample_table, созданная из Spark.createOrReplaceTempView, считается временной таблицей, 
а таблица Hive — управляемой таблицей(https://sparkbyexamples.com/apache-hive/difference-between-hive-internal-tables-and-external-tables/).

5. Часто используемые методы SparkSession

+--------------------------+---------------------------------------------------------------------------------------------------------------+
| 
METHOD	DESCRIPTION
































